{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data into a Pandas dataframe\n",
    "df = pd.read_csv(\"ara_eng.txt\", sep=\"\\t\", header=None, names=[\"English\",\"Arabic\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['English'] = df['English'].str.lower()\n",
    "# Remove special characters from English text\n",
    "df['English'] = df['English'].apply(lambda x: re.sub(r'[^a-zA-Z\\s]', '', x))\n",
    "\n",
    "# Remove special characters from Arabic text\n",
    "df['Arabic'] = df['Arabic'].apply(lambda x: re.sub(r'[^\\u0621-\\u064A\\s]', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_en = []\n",
    "attention_masks_en = []\n",
    "\n",
    "input_ids_ar = []\n",
    "attention_masks_ar = []\n",
    "\n",
    "for eng_text, ar_text in zip(df['English'], df['Arabic']):\n",
    "    encoded_en = tokenizer.encode_plus(\n",
    "        eng_text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=128,\n",
    "        pad_to_max_length=True,\n",
    "        return_attention_mask=True\n",
    "    )\n",
    "    \n",
    "    encoded_ar = tokenizer.encode_plus(\n",
    "        ar_text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=128,\n",
    "        pad_to_max_length=True,\n",
    "        return_attention_mask=True\n",
    "    )\n",
    "    \n",
    "    input_ids_en.append(encoded_en['input_ids'])\n",
    "    attention_masks_en.append(encoded_en['attention_mask'])\n",
    "    \n",
    "    input_ids_ar.append(encoded_ar['input_ids'])\n",
    "    attention_masks_ar.append(encoded_ar['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_en = torch.tensor(input_ids_en)\n",
    "attention_masks_en = torch.tensor(attention_masks_en)\n",
    "\n",
    "input_ids_ar = torch.tensor(input_ids_ar)\n",
    "attention_masks_ar = torch.tensor(attention_masks_ar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(input_ids_en, attention_masks_en, input_ids_ar, attention_masks_ar)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from Translation_Transformers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Transformer model\n",
    "model = Transformer(src_vocab_size=tokenizer.vocab_size, \n",
    "                    tgt_vocab_size=tokenizer.vocab_size,\n",
    "                    d_model=512, \n",
    "                    num_layers=6, \n",
    "                    num_heads=8, \n",
    "                    d_ff=2048, \n",
    "                    max_len=128,\n",
    "                    dropout=0.1)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Training function\n",
    "def train(model, dataloader, num_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        loop = tqdm(dataloader, leave=True)\n",
    "        for batch in loop:\n",
    "            input_ids_en, attention_masks_en, input_ids_ar, attention_masks_ar = batch\n",
    "            \n",
    "            # Use the provided attention masks\n",
    "            src_mask = attention_masks_en.unsqueeze(1).unsqueeze(2)\n",
    "            tgt_mask = attention_masks_ar[:, :-1].unsqueeze(1).unsqueeze(2)\n",
    "            \n",
    "            output = model(input_ids_en, input_ids_ar[:, :-1], \n",
    "                           src_mask=src_mask, \n",
    "                           tgt_mask=tgt_mask)\n",
    "            \n",
    "            output = output.reshape(-1, output.size(-1))\n",
    "            labels = input_ids_ar[:, 1:].contiguous().view(-1)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            loop.set_postfix(loss=loss.item())\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# Train the model\n",
    "train(model, dataloader, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "            \n",
    "# Generate subsequent mask for target sequence\n",
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = torch.triu(torch.ones(sz, sz) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
